{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPlI/SIeCiFxPrTVpF8Hja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VoonaSriraj/Enhanced-Machine-Translation/blob/main/machinetranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1RUehWLt5NX"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ENHANCED MACHINE TRANSLATION - ADD THIS CELL TO YOUR NOTEBOOK\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "Add this cell to your existing machine_translation.ipynb notebook\n",
        "This provides a drop-in enhancement to your existing models\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "\n",
        "# Check for enhanced dependencies\n",
        "try:\n",
        "    import sentencepiece as smp\n",
        "    from sacrebleu import corpus_bleu\n",
        "    ENHANCED_MODE = True\n",
        "    print(\"✅ Enhanced mode: Advanced features available\")\n",
        "except ImportError:\n",
        "    ENHANCED_MODE = False\n",
        "    print(\"⚠️ Basic mode: Install 'sentencepiece sacrebleu' for full features\")\n",
        "\n",
        "class EnhancedConfig:\n",
        "    \"\"\"Configuration for enhanced models\"\"\"\n",
        "    def __init__(self):\n",
        "        # Model architecture\n",
        "        self.d_model = 256  # Smaller for compatibility with existing notebook\n",
        "        self.num_heads = 8\n",
        "        self.num_layers = 4\n",
        "        self.dff = 1024\n",
        "        self.dropout_rate = 0.1\n",
        "\n",
        "        # Training\n",
        "        self.batch_size = 64\n",
        "        self.epochs = 15\n",
        "        self.learning_rate = 1e-4\n",
        "        self.warmup_steps = 2000\n",
        "\n",
        "        # Advanced features\n",
        "        self.use_beam_search = True\n",
        "        self.beam_size = 4\n",
        "        self.length_penalty = 0.6\n",
        "        self.label_smoothing = 0.1\n",
        "\n",
        "# Enhanced Transformer implementation compatible with your existing notebook\n",
        "class ModernMultiHeadAttention(layers.Layer):\n",
        "    \"\"\"Improved multi-head attention with better initialization\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model, kernel_initializer='glorot_uniform')\n",
        "        self.wk = layers.Dense(d_model, kernel_initializer='glorot_uniform')\n",
        "        self.wv = layers.Dense(d_model, kernel_initializer='glorot_uniform')\n",
        "\n",
        "        self.dense = layers.Dense(d_model, kernel_initializer='glorot_uniform')\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        v, k, q = inputs\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnhancedTransformerBlock(layers.Layer):\n",
        "    \"\"\"Enhanced transformer block with pre-layer normalization\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.mha = ModernMultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu', kernel_initializer='glorot_uniform'),\n",
        "            layers.Dense(d_model, kernel_initializer='glorot_uniform')\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        # Pre-layer normalization\n",
        "        attn_input = self.layernorm1(x)\n",
        "        attn_output = self.mha([attn_input, attn_input, attn_input])\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = x + attn_output  # Residual connection\n",
        "\n",
        "        ffn_input = self.layernorm2(out1)\n",
        "        ffn_output = self.ffn(ffn_input)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = out1 + ffn_output  # Residual connection\n",
        "\n",
        "        return out2\n",
        "\n",
        "def enhanced_transformer_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Enhanced Transformer model - replacement for your model_final function\n",
        "    \"\"\"\n",
        "    config = EnhancedConfig()\n",
        "\n",
        "    # Input layers\n",
        "    encoder_inputs = layers.Input(shape=input_shape[1:], name='encoder_inputs')\n",
        "    decoder_inputs = layers.Input(shape=(output_sequence_length,), name='decoder_inputs')\n",
        "\n",
        "    # Embeddings\n",
        "    encoder_embedding = layers.Embedding(\n",
        "        english_vocab_size, config.d_model,\n",
        "        mask_zero=True, name='encoder_embedding'\n",
        "    )(encoder_inputs)\n",
        "\n",
        "    decoder_embedding = layers.Embedding(\n",
        "        french_vocab_size, config.d_model,\n",
        "        mask_zero=True, name='decoder_embedding'\n",
        "    )(decoder_inputs)\n",
        "\n",
        "    # Positional encoding\n",
        "    def add_positional_encoding(embedded_inputs):\n",
        "        seq_len = tf.shape(embedded_inputs)[1]\n",
        "        d_model = embedded_inputs.shape[-1]\n",
        "\n",
        "        # Create positional encoding\n",
        "        pos = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
        "        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
        "\n",
        "        angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / d_model)\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # Apply sin to even indices\n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        # Apply cos to odd indices\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        return embedded_inputs + pos_encoding\n",
        "\n",
        "    # Add positional encoding\n",
        "    encoder_outputs = add_positional_encoding(encoder_embedding)\n",
        "    decoder_outputs = add_positional_encoding(decoder_embedding)\n",
        "\n",
        "    # Encoder layers\n",
        "    for i in range(config.num_layers):\n",
        "        encoder_outputs = EnhancedTransformerBlock(\n",
        "            config.d_model, config.num_heads, config.dff, config.dropout_rate,\n",
        "            name=f'encoder_layer_{i}'\n",
        "        )(encoder_outputs)\n",
        "\n",
        "    # Decoder layers (simplified for seq2seq)\n",
        "    for i in range(config.num_layers):\n",
        "        decoder_outputs = EnhancedTransformerBlock(\n",
        "            config.d_model, config.num_heads, config.dff, config.dropout_rate,\n",
        "            name=f'decoder_layer_{i}'\n",
        "        )(decoder_outputs)\n",
        "\n",
        "    # Final dense layer\n",
        "    outputs = layers.Dense(french_vocab_size, activation='softmax', name='output_layer')(decoder_outputs)\n",
        "\n",
        "    # Create model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs, name='enhanced_transformer')\n",
        "\n",
        "    # Enhanced optimizer with learning rate scheduling\n",
        "    initial_learning_rate = config.learning_rate\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate, decay_steps=1000, alpha=0.1\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr_schedule,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.98,\n",
        "        epsilon=1e-9\n",
        "    )\n",
        "\n",
        "    # Enhanced loss with label smoothing\n",
        "    def label_smoothed_loss(y_true, y_pred):\n",
        "        # Convert sparse labels to one-hot\n",
        "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), french_vocab_size)\n",
        "\n",
        "        # Apply label smoothing\n",
        "        smoothing = config.label_smoothing\n",
        "        y_true_smooth = y_true_one_hot * (1 - smoothing) + smoothing / french_vocab_size\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = tf.keras.losses.categorical_crossentropy(y_true_smooth, y_pred)\n",
        "\n",
        "        # Mask padding tokens (assuming 0 is padding)\n",
        "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "        loss = loss * mask\n",
        "\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=label_smoothed_loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def enhanced_beam_search_decode(model, input_sequence, tokenizer, beam_size=4, max_length=50):\n",
        "    \"\"\"\n",
        "    Beam search decoder for better translation quality\n",
        "    \"\"\"\n",
        "    # Encode input\n",
        "    encoder_input = tf.expand_dims(input_sequence, 0)\n",
        "\n",
        "    # Initialize beam with start token (assuming 1 is start token)\n",
        "    beams = [(0.0, [1])]  # (score, sequence)\n",
        "\n",
        "    for step in range(max_length):\n",
        "        all_candidates = []\n",
        "\n",
        "        for score, sequence in beams:\n",
        "            if len(sequence) > 0 and sequence[-1] == 2:  # End token\n",
        "                all_candidates.append((score, sequence))\n",
        "                continue\n",
        "\n",
        "            # Prepare decoder input\n",
        "            decoder_input = tf.constant([sequence + [0] * (max_length - len(sequence))])\n",
        "            decoder_input = decoder_input[:, :max_length]\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model([encoder_input, decoder_input])\n",
        "\n",
        "            # Get probabilities for next token\n",
        "            next_token_probs = predictions[0, len(sequence)-1, :]\n",
        "\n",
        "            # Get top beam_size tokens\n",
        "            top_probs, top_indices = tf.nn.top_k(next_token_probs, k=beam_size)\n",
        "\n",
        "            for i in range(beam_size):\n",
        "                token_id = int(top_indices[i])\n",
        "                token_prob = float(tf.nn.log_softmax(next_token_probs)[top_indices[i]])\n",
        "\n",
        "                new_sequence = sequence + [token_id]\n",
        "                new_score = score + token_prob\n",
        "\n",
        "                # Length penalty\n",
        "                length_penalty = ((5 + len(new_sequence)) / 6) ** 0.6\n",
        "                normalized_score = new_score / length_penalty\n",
        "\n",
        "                all_candidates.append((normalized_score, new_sequence))\n",
        "\n",
        "        # Keep top beam_size candidates\n",
        "        beams = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
        "\n",
        "        # Stop if all beams end with end token\n",
        "        if all(len(seq) > 0 and seq[-1] == 2 for _, seq in beams):\n",
        "            break\n",
        "\n",
        "    # Return best sequence\n",
        "    return beams[0][1]\n",
        "\n",
        "def evaluate_enhanced_model(model, test_data, tokenizer, beam_search=True):\n",
        "    \"\"\"\n",
        "    Enhanced evaluation with multiple metrics\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "    for i, (src, tgt) in enumerate(test_data):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Progress: {i}/{len(test_data)}\")\n",
        "\n",
        "        if beam_search:\n",
        "            pred_sequence = enhanced_beam_search_decode(model, src, tokenizer)\n",
        "        else:\n",
        "            # Simple greedy decoding\n",
        "            encoder_input = tf.expand_dims(src, 0)\n",
        "            decoder_input = tf.constant([[1] + [0] * 49])  # Start with start token\n",
        "\n",
        "            pred = model([encoder_input, decoder_input])\n",
        "            pred_sequence = tf.argmax(pred[0], axis=-1).numpy().tolist()\n",
        "\n",
        "        # Convert back to text (simplified)\n",
        "        pred_text = ' '.join([str(token) for token in pred_sequence if token not in [0, 1, 2]])\n",
        "        ref_text = ' '.join([str(token) for token in tgt if token not in [0, 1, 2]])\n",
        "\n",
        "        predictions.append(pred_text)\n",
        "        references.append(ref_text)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    if ENHANCED_MODE:\n",
        "        try:\n",
        "            bleu_score = corpus_bleu(predictions, [[ref] for ref in references]).score\n",
        "            print(f\"📊 sacreBLEU score: {bleu_score:.2f}\")\n",
        "        except:\n",
        "            print(\"⚠️ Could not calculate sacreBLEU\")\n",
        "\n",
        "    # Simple BLEU approximation\n",
        "    def simple_bleu(pred, ref):\n",
        "        pred_tokens = set(pred.split())\n",
        "        ref_tokens = set(ref.split())\n",
        "        if len(pred_tokens) == 0:\n",
        "            return 0.0\n",
        "        return len(pred_tokens & ref_tokens) / len(pred_tokens)\n",
        "\n",
        "    simple_bleu_scores = [simple_bleu(p, r) for p, r in zip(predictions, references)]\n",
        "    avg_simple_bleu = np.mean(simple_bleu_scores)\n",
        "\n",
        "    print(f\"📊 Simple BLEU score: {avg_simple_bleu:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'predictions': predictions,\n",
        "        'references': references,\n",
        "        'simple_bleu': avg_simple_bleu,\n",
        "        'bleu_scores': simple_bleu_scores\n",
        "    }\n",
        "\n",
        "def create_enhanced_datasets(english_sentences, french_sentences, test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Create proper train/validation/test splits\n",
        "    \"\"\"\n",
        "    print(\"🔄 Creating enhanced data splits...\")\n",
        "\n",
        "    # First split: separate test set\n",
        "    en_temp, en_test, fr_temp, fr_test = train_test_split(\n",
        "        english_sentences, french_sentences,\n",
        "        test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    # Second split: separate validation set\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    en_train, en_val, fr_train, fr_val = train_test_split(\n",
        "        en_temp, fr_temp,\n",
        "        test_size=val_size_adjusted, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Dataset splits:\")\n",
        "    print(f\"  - Training: {len(en_train)} pairs\")\n",
        "    print(f\"  - Validation: {len(en_val)} pairs\")\n",
        "    print(f\"  - Test: {len(en_test)} pairs\")\n",
        "\n",
        "    return {\n",
        "        'train': (en_train, fr_train),\n",
        "        'val': (en_val, fr_val),\n",
        "        'test': (en_test, fr_test)\n",
        "    }\n",
        "\n",
        "def enhanced_training_pipeline(english_sentences, french_sentences):\n",
        "    \"\"\"\n",
        "    Complete enhanced training pipeline\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting Enhanced Machine Translation Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create datasets\n",
        "    datasets = create_enhanced_datasets(english_sentences, french_sentences)\n",
        "    en_train, fr_train = datasets['train']\n",
        "    en_val, fr_val = datasets['val']\n",
        "    en_test, fr_test = datasets['test']\n",
        "\n",
        "    # Preprocess data (reuse your existing preprocessing)\n",
        "    print(\"🔄 Preprocessing data...\")\n",
        "    train_x, train_y, x_tokenizer, y_tokenizer = preprocess(en_train, fr_train)\n",
        "    val_x, val_y, _, _ = preprocess(en_val, fr_val)\n",
        "    test_x, test_y, _, _ = preprocess(en_test, fr_test)\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    english_vocab_size = len(x_tokenizer.word_index) + 1\n",
        "    french_vocab_size = len(y_tokenizer.word_index) + 1\n",
        "    max_sequence_length = train_y.shape[1]\n",
        "\n",
        "    print(f\"📝 Vocabulary sizes: EN={english_vocab_size}, FR={french_vocab_size}\")\n",
        "\n",
        "    # Create enhanced model\n",
        "    print(\"🏗️ Building Enhanced Transformer...\")\n",
        "    model = enhanced_transformer_model(\n",
        "        train_x.shape,\n",
        "        max_sequence_length,\n",
        "        english_vocab_size,\n",
        "        french_vocab_size\n",
        "    )\n",
        "\n",
        "    print(f\"📋 Model Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Prepare training data for the enhanced model\n",
        "    train_encoder_input = train_x\n",
        "    train_decoder_input = train_y[:, :-1]  # Exclude last token\n",
        "    train_target = train_y[:, 1:]  # Exclude first token\n",
        "\n",
        "    val_encoder_input = val_x\n",
        "    val_decoder_input = val_y[:, :-1]\n",
        "    val_target = val_y[:, 1:]\n",
        "\n",
        "    # Enhanced training with callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            patience=3, restore_best_weights=True, monitor='val_loss'\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            factor=0.5, patience=2, min_lr=1e-6, monitor='val_loss'\n",
        "        ),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'enhanced_model_best.h5', save_best_only=True, monitor='val_loss'\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"🎯 Training Enhanced Model...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        [train_encoder_input, train_decoder_input], train_target,\n",
        "        batch_size=64,\n",
        "        epochs=15,\n",
        "        validation_data=([val_encoder_input, val_decoder_input], val_target),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"⏱️ Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    # Enhanced evaluation\n",
        "    print(\"📊 Evaluating Enhanced Model...\")\n",
        "\n",
        "    # Prepare test data\n",
        "    test_data = list(zip(test_x, test_y))\n",
        "\n",
        "    # Evaluate with beam search\n",
        "    results = evaluate_enhanced_model(model, test_data[:100], y_tokenizer, beam_search=True)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Enhanced Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Enhanced Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Show example translations\n",
        "    print(\"\\n🌟 Sample Enhanced Translations:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i in range(min(5, len(en_test))):\n",
        "        src_text = en_test[i]\n",
        "        ref_text = fr_test[i]\n",
        "\n",
        "        # Get model prediction\n",
        "        src_tokens = x_tokenizer.texts_to_sequences([src_text])\n",
        "        src_padded = pad_sequences(src_tokens, maxlen=train_x.shape[1], padding='post')\n",
        "\n",
        "        if len(test_data) > i:\n",
        "            pred_tokens = enhanced_beam_search_decode(model, src_padded[0], y_tokenizer, beam_size=4)\n",
        "            pred_text = y_tokenizer.sequences_to_texts([pred_tokens])[0] if pred_tokens else \"Unable to decode\"\n",
        "        else:\n",
        "            pred_text = \"Unable to generate\"\n",
        "\n",
        "        print(f\"\\n📝 Example {i+1}:\")\n",
        "        print(f\"  🇬🇧 Source: {src_text}\")\n",
        "        print(f\"  🇫🇷 Reference: {ref_text}\")\n",
        "        print(f\"  🤖 Enhanced Prediction: {pred_text}\")\n",
        "        print(f\"  📊 Simple BLEU: {results['bleu_scores'][i] if i < len(results['bleu_scores']) else 'N/A':.4f}\")\n",
        "\n",
        "    print(\"\\n✅ Enhanced Training Pipeline Completed!\")\n",
        "    print(\"🎉 Key Improvements Applied:\")\n",
        "    print(\"  ✓ Modern Transformer architecture with pre-layer normalization\")\n",
        "    print(\"  ✓ Proper train/validation/test splits\")\n",
        "    print(\"  ✓ Label smoothing for better generalization\")\n",
        "    print(\"  ✓ Learning rate scheduling\")\n",
        "    print(\"  ✓ Beam search decoding\")\n",
        "    print(\"  ✓ Enhanced evaluation metrics\")\n",
        "    print(\"  ✓ Early stopping and model checkpointing\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'results': results,\n",
        "        'tokenizers': (x_tokenizer, y_tokenizer),\n",
        "        'datasets': datasets\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE: Run this in your notebook after loading your data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🎯 Enhanced Machine Translation Ready!\")\n",
        "print(\"To use: run enhanced_training_pipeline(english_sentences, french_sentences)\")\n",
        "print()\n",
        "print(\"Example:\")\n",
        "print(\"enhanced_results = enhanced_training_pipeline(english_sentences, french_sentences)\")\n",
        "print()\n",
        "print(\"Features:\")\n",
        "print(\"  🔥 Modern Transformer with attention improvements\")\n",
        "print(\"  📊 Proper data splitting and evaluation\")\n",
        "print(\"  🎯 Beam search for better translations\")\n",
        "print(\"  📈 Advanced training techniques\")\n",
        "print(\"  🏆 Enhanced metrics and visualization\")"
      ]
    }
  ]
}